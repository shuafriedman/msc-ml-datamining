{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/shua/Desktop/msc-ml-datamining/Machine_Learning/project/v2/data/prepared_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset = ['prev_sale_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#drop columns that are address related\n",
    "train_df = df.loc[:, ~df.columns.str.contains('^address')]\n",
    "train_df = train_df.drop(['street', 'city', 'state', 'zip'], axis = 1)\n",
    "#drop years (if calculated the age)\n",
    "train_df = train_df.drop(['prev_sale_price', 'sale.lastSale.saleDate', 'sale.priorSale.saleDate', 'building.yearBuilt', 'sold_month'], axis = 1)\n",
    "#split target\n",
    "target = train_df.loc[:, 'sale.lastSale.price']\n",
    "train_df = train_df.drop(['sale.lastSale.price'], axis = 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_df, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Missing Values: Fill missing values based off of column-wise aggregations on the training set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we will any remaining empty values based on column wise aggregations (such as mode or mean). Best practice is to just do this for the training sets, and use the modes and mean (or other metrics) of the training set for the test set (utilising the fit_transform methods of sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# #create a Datafill class to fill in the \"small_missing\" missing values with the mode from the training set, even on prediction\n",
    "# class DataFill(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def fit(self, X, y = None):\n",
    "#         self.columns = X.columns\n",
    "#         self.modes_ = {}\n",
    "#         for col in self.columns:\n",
    "#             self.modes_[col] = X[col].mode()[0]\n",
    "#         return self\n",
    "#     def transform(self, X, y = None):\n",
    "#         X = X.copy()\n",
    "#         for col in self.columns:\n",
    "#             X[col].fillna(self.modes_[col], inplace = True)\n",
    "#         return X\n",
    "\n",
    "# fill = DataFill()\n",
    "# fill.fit(df[small_missing])\n",
    "# df[small_missing] = fill.transform(df[small_missing])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Min-max instead of standard scaling, because of the many one-hot encodings (need a source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the log of the target\n",
    "# y_train = np.log(y_train)\n",
    "# y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "# imputer.fit(x_train)\n",
    "# x_train_impute = pd.DataFrame(imputer.transform(x_train), columns=train_df.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation: Use KNN to fill in the rest of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement knn imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputer.fit(x_train)\n",
    "x_train = pd.DataFrame(imputer.transform(x_train), columns=train_df.columns)\n",
    "x_test = pd.DataFrame(imputer.transform(x_test), columns=train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out the columns that have null values\n",
    "x_train.isnull().sum()[x_train.isnull().sum() > 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Need to group unique houses into same train and test splits. Otherwise, risk overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lev_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:25:18) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08c4878952a3fd7112a89f2b48435e7b10e94f649272565e5b49c018510cb2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
