{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/07 10:05:07 WARN Utils: Your hostname, shua-VivoBook-ASUSLaptop-X513EAN-K513EA resolves to a loopback address: 127.0.1.1; using 10.0.0.17 instead (on interface wlo1)\n",
      "23/07/07 10:05:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/07 10:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = sc.textFile(\"data/ads.txt\").map(lambda x : x.split(',')).map(lambda x : [x[1], x[0]])\n",
    "channels = sc.textFile(\"data/channels.txt\").map(lambda x : x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ads \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mtextFile(\u001b[39m\"\u001b[39;49m\u001b[39mdata/ads.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x : x\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m      2\u001b[0m channels \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mtextFile(\u001b[39m\"\u001b[39m\u001b[39mdata/channels.txt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mads\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x : x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39mrdd())\n\u001b[1;32m   1815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5439\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5441\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   5442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   5443\u001b[0m )\n\u001b[1;32m   5445\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   5447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   5448\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5243\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   5242\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5243\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSimplePythonFunction(\n\u001b[1;32m   5244\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[1;32m   5245\u001b[0m     env,\n\u001b[1;32m   5246\u001b[0m     includes,\n\u001b[1;32m   5247\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[1;32m   5248\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[1;32m   5249\u001b[0m     broadcast_vars,\n\u001b[1;32m   5250\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[1;32m   5251\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "input = ads.join(channels).sortBy(lambda x: x[1][0])\n",
    "output = input.map(lambda x: x[1]).reduceByKey(lambda a,b : int(a)+int(b)).sortBy(lambda x: x[1], ascending=False).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pair RDD with the key being the ad ID and the value being the channel ID \n",
    "# example of the add.txt file is:\n",
    "##Air Taser Model 34000,HVT\n",
    "##Air Taser Model 34000,TLK\n",
    "##Air Taser Model 34000,IJU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inout \u001b[39m=\u001b[39m ads\u001b[39m.\u001b[39;49mjoin(channels)\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:3651\u001b[0m, in \u001b[0;36mRDD.join\u001b[0;34m(self, other, numPartitions)\u001b[0m\n\u001b[1;32m   3607\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\n\u001b[1;32m   3608\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[Tuple[K, V]]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3609\u001b[0m     other: \u001b[39m\"\u001b[39m\u001b[39mRDD[Tuple[K, U]]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3610\u001b[0m     numPartitions: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   3611\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRDD[Tuple[K, Tuple[V, U]]]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3612\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3613\u001b[0m \u001b[39m    Return an RDD containing all pairs of elements with matching keys in\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m \u001b[39m    `self` and `other`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3649\u001b[0m \u001b[39m    [('a', (1, 2)), ('a', (1, 3))]\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3651\u001b[0m     \u001b[39mreturn\u001b[39;00m python_join(\u001b[39mself\u001b[39;49m, other, numPartitions)\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/join.py:54\u001b[0m, in \u001b[0;36mpython_join\u001b[0;34m(rdd, other, numPartitions)\u001b[0m\n\u001b[1;32m     51\u001b[0m             wbuf\u001b[39m.\u001b[39mappend(v)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m ((v, w) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vbuf \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m wbuf)\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m _do_python_join(rdd, other, numPartitions, dispatch)\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/join.py:41\u001b[0m, in \u001b[0;36m_do_python_join\u001b[0;34m(rdd, other, numPartitions, dispatch)\u001b[0m\n\u001b[1;32m     39\u001b[0m vs \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapValues(\u001b[39mlambda\u001b[39;00m v: (\u001b[39m1\u001b[39m, v))\n\u001b[1;32m     40\u001b[0m ws \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mmapValues(\u001b[39mlambda\u001b[39;00m v: (\u001b[39m2\u001b[39m, v))\n\u001b[0;32m---> 41\u001b[0m \u001b[39mreturn\u001b[39;00m vs\u001b[39m.\u001b[39;49munion(ws)\u001b[39m.\u001b[39mgroupByKey(numPartitions)\u001b[39m.\u001b[39mflatMapValues(\u001b[39mlambda\u001b[39;00m x: dispatch(x\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()))\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:1246\u001b[0m, in \u001b[0;36mRDD.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[39mReturn the union of this RDD and another one.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39m[1, 1, 2, 3, 1, 1, 2, 3]\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer \u001b[39m==\u001b[39m other\u001b[39m.\u001b[39m_jrdd_deserializer:\n\u001b[1;32m   1245\u001b[0m     rdd: \u001b[39m\"\u001b[39m\u001b[39mRDD[Union[T, U]]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m RDD(\n\u001b[0;32m-> 1246\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39munion(other\u001b[39m.\u001b[39m_jrdd), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer\n\u001b[1;32m   1247\u001b[0m     )\n\u001b[1;32m   1248\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m     \u001b[39m# These RDDs contain data in different serialized formats, so we\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# must normalize them to the default serializer.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     self_copy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reserialize()\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5439\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5441\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   5442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   5443\u001b[0m )\n\u001b[1;32m   5445\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   5447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   5448\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/msc-ml-datamining/BigData/week5_spark/setup_spark/.venv/lib/python3.10/site-packages/pyspark/rdd.py:5243\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   5242\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5243\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSimplePythonFunction(\n\u001b[1;32m   5244\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[1;32m   5245\u001b[0m     env,\n\u001b[1;32m   5246\u001b[0m     includes,\n\u001b[1;32m   5247\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[1;32m   5248\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[1;32m   5249\u001b[0m     broadcast_vars,\n\u001b[1;32m   5250\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[1;32m   5251\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "input = ads.join(channels).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
