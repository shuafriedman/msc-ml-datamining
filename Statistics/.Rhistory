install.packages('MASS')
library(MASS)
Book1
install.packages("ISLR")
library(ISLR)
data=Smarket
library(MASS)
#split the data to before/after 2005
train=(data$Year<2005)
table(train)
datatrain=data[train,]
datatest=data[!train,]
columns(datatrain)
datatrain
summary(datatrain)
colnames(datatrain)
colnames(datatrain)
colnames(datatrain)
colnames(datatrain)
lda.model
lda.model=lda(Direction~Lag1+Lag2,data=datatrain)
lda.model
#predictions
lda.pred=predict(lda.model,datatest[,1:8])
lda.pred$class
table(lda.pred$class,datatest[,9])
table(lda)
table(lda.pred)
table(lda.pred, datatest[.9])
lda.pred$class
#predictions
datatest
#predictions
datatest[1]
#predictions
datatest[,1]
#predictions
datatest[1:18]
#predictions
datatest[1:4]
#predictions
datatest[1:4, 1:4]
#predictions
datatest[, 1]
#predictions
datatest[, 2]
#predictions
datatest[, 2:3]
#predictions
datatest[2:3]
#predictions
datatest[, 1:8]
#predictions
datatest[1:8]
#predictions
lda.pred=predict(lda.model,datatest[1:8])
lda.pred$class
#predictions
lda.pred=predict(lda.model,datatest[,1:8])
lda.pred$class
mean(lda.pred$class==datatest[,9])
#qda
qda.model=qda(Direction~Lag1+Lag2,data=datatrain)
qda.model
#qda-- Quadratic Dicrimminant Analysis
qda.model=qda(Direction~Lag1+Lag2,data=datatrain)
qda.model
qda.pred=predict(qda.model,datatest[,1:8])
qda.pred$class
table(qda.pred$class,datatest[,9])
mean(qda.pred$class==datatest[,9])
tar3_q1_data
library(readxl)
tar3_q1_data <- read_excel("hypothesis_testing-Anova/tar3_q1_data.xlsx")
View(tar3_q1_data)
tar3_q1_data
p_less_then_20 <- pnorm(20,33,7)
p_less_then_20
p_20_to_30 <- pnorm(30,33,7) - pnorm(20,33,7)
p_20_to_30
p_30_to_40<- pnorm(40,33,7) - pnorm(30,33,7)
p_30_to_40
p_more_then_40 <- 1 - pnorm(40,33,7)
p_more_then_40
#chisq.test(tar3_q1_data$num,p=c(0.032,0.302,0.507,0.159))
chisq.test(tar3_q1_data$num,p=c(p_less_then_20,p_20_to_30,p_30_to_40,p_more_then_40))
qchisq(.95,5)
chisq.test(tar3_q1_data$num,p=c(p_less_then_20,p_20_to_30,p_30_to_40,p_more_then_40))
qchisq(.95,5)
rnorm(1000, 33, 7 )
rand_norm <-rnorm(1000, 33, 7)
p_less_then_20 <- pnorm(20,33,7)
p_less_then_20
p_20_to_30 <- pnorm(30,33,7) - pnorm(20,33,7)
p_20_to_30
c(5)*10
1 - pnorm(19, mean=17.46, sd=sqrt(375.67))
qchisq(.95,5)
qchisq(.95,4)
qchisq(.95,3)
qchisq(.95,6)
qchisq(.95,5)
#chisq.test(tar3_q1_data$num,p=c(0.032,0.302,0.507,0.159))
chisq.test(tar3_q1_data$num,p=c(p_less_then_20,p_20_to_30,p_30_to_40,p_more_then_40))
## Goodness of Fit
observed = c(94, 93, 112, 101, 104, 95, 100, 99, 108, 94)
prob = rep(1/length(observed), length(observed))
chisq.test(observed, p=prob)
chisq.test(observed, p=prob)
install.packages('power')
install.packages('pwr')
library('pwr')
pwr.t.test(d = 0.5/1, power = 0.9, alternative = "greater", sig.level = 0.05)
pwr.t.test(d = 0.5/1, power = 0.9, type='one.sample', alternative = "greater", sig.level = 0.05)
pcr.fit=pcr(Salary~.,data=data1,scale=TRUE)
install.packages('ISLR')
library(ISLR)
head(Hitters)
str(Hitters)
data1=na.omit(Hitters) ### Can also set na.action in the prcomp formula below
a=c(1:13,16,17,18)
b=prcomp(data1[,a],scale=TRUE) ### PRincipal Component Analysis
c=b$sdev^2 #get the variances
c
for (i in 1:16){print(sum(c[1:i])/sum(c))} #cumulative sum of the variances for each feature
# the rest of this scrip was not coverred in class
# fitting the model-- Partial Least Squares regression with PCA (built into pcr function)
install.packages('pls')
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~.,data=data1,scale=TRUE)
summary(pcr.fit)
validationplot(pcr.fit,val.type = "MSEP")
install.packages("ISLR")
head(Hitters)
str(Hitters)
data1=na.omit(Hitters) ### Can also set na.action in the prcomp formula below
a=c(1:13,16,17,18)
b=prcomp(data1[,a],scale=TRUE) ### PRincipal Component Analysis
c=b$sdev^2 #get the variances
c
for (i in 1:16){print(sum(c[1:i])/sum(c))}
b[,1:4]
# the rest of this scrip was not coverred in class
b[,4]
# the rest of this scrip was not coverred in class
b
# the rest of this scrip was not coverred in class
summary(b)
data(mtcars)
my_pca <- prcomp(mtcars, scale=TRUE, center= TRUE, retx=T)
summary(my_pca)
my_pca
my_pca$rotation
my_pca$rotation
dim(my_pca$x)
my_pca$x
dim(data)
dim(my_pca$x)
my_pca$x
mtcars
my_pca$x[,1:4]
my_pca <- prcomp(mtcars, scale=TRUE, center= TRUE, retx=T)
summary(my_pca)
my_pca$sdev
# Compute variance
my_pca.var <- my_pca$sdev ^ 2
my_pca.var
which(cumsum(propve) >= 0.9)[1]
my_pca.var <- my_pca$sdev ^ 2
propve <- my_pca.var / sum(my_pca.var)
propve
which(cumsum(propve) >= 0.9)[1]
train.data <- data.frame(disp = mtcars$disp, my_pca$x[, 1:which(cumsum(propve) >= 0.9)[1]])
train.data
data(mtcars)
my_pca <- prcomp(mtcars, scale=TRUE, center= TRUE, retx=T)
summary(my_pca)
my_pca$sdev
# Compute variance
my_pca.var <- my_pca$sdev ^ 2
propve <- my_pca.var / sum(my_pca.var)
propve
which(cumsum(propve) >= 0.9)[1]
train.data <- data.frame(disp = mtcars$disp, my_pca$x[, 1:which(cumsum(propve) >= 0.9)[1]])
train.data
library(readxl)
final1data1 <- read_excel("TEST/final1data1.xlsx")
View(final1data1)
t.test(final1data1$A, final1data1$B)
x = c(150, 100, 50)
x
x = c(150, 100, 50)
y = c(100, 400, 200)
chisq.test(x, y)
x = c(150, 100, 50)
y = c(100, 400, 200)
data(x, y)
x = c(150, 100, 50)
y = c(100, 400, 200)
rbind((x, y))
x = c(150, 100, 50)
y = c(100, 400, 200)
rbind(x, y)
x = c(150, 100, 50)
y = c(100, 400, 200)
data = rbind(x, y)
x = c(150, 100, 50)
y = c(100, 400, 200)
data = rbind(x, y)
chisq.test(data)
library(readxl)
final1data2 <- read_excel("TEST/final1data2.xlsx")
View(final1data2)
aov(זן א`., ata = final1data2)
aov(זן א~., ata = final1data2)
ao(data = final1data2)
source("~/.active-rstudio-document", echo=TRUE)
data = final1data2
colnames(data) <- c("x","Y", "Z")
data
aov(X~.,data = final1data2)
data = final1data2
colnames(data) <- c("x","y", "z")
data
aov(X~.,data = final1data2)
source("~/.active-rstudio-document", echo=TRUE)
aov(x~.,data = final1data2)
aov(x~.,data = data)
summary(aov(x~.,data = data))
TukeyHSD(aov(x~.,data = data))
stack(final1data2)
stack(final1data2)
summary(aov(values~ind, data = stack(final1data2)))
aov(values~ind, data = stack(final1data2))
TukeyHSD(aov(values~ind, data = stack(final1data2)))
TukeyHSD(aov(y~.,data = data))
summary((aov(y~.,data = data))
summary((aov(y~.,data = data)))
stack(final1data2)
summary((aov(y~.,data = data)))
summary((aov(z~.,data = data)))
summary((aov(y~.,data = data)))
data = data(mtcars)
model=lm(mpg~.,data=data)
data
model=lm(mpg~.,data=mtcars)
summary(model)
model
source("~/.active-rstudio-document", echo=TRUE)
both=step(intercept_only,direction = "both",scope=formula(model),trace = 1)
y = c(100, 400, 200)
source("~/.active-rstudio-document", echo=TRUE)
summary(aov(values~ind, data = stack(final1data2)))
TukeyHSD(aov(values~ind, data = stack(final1data2)))
t.test(final1data1$A, final1data1$B)
t.test(final1data1$A, final1data1$B, var.equal = T)
t.test(final1data1$A, final1data1$B)
qda.model=qda(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=data)
data=iris
head(data)
install.packages("MASS")
library(MASS)
qda.model=qda(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=data)
qda.pred=predict(qda.model,data[,1:4])
qda.pred$posterior
qda.pred$class
table(qda.pred$class,data[,5])
mean(qda.pred$class==data[,5])
logitmodel=glm(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=data)
source("C:/Users/sfrie/OneDrive/Desktop/msc-ml-datamining/Statistics/TEST/shua-test.R", echo=TRUE)
data=iris
head(data)
logitmodel=glm(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=data)
source("C:/Users/sfrie/OneDrive/Desktop/msc-ml-datamining/Statistics/TEST/shua-test.R", echo=TRUE)
data$Species
logitmodel=glm(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=data, family='gaussian')
logitmodel=glm(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=data, family='binomial')
table(logitmodel.pred$class,data[,5])
predict(logitmodel, data[,5], type='response')
predict(logitmodel, data[,1:4], type='response')
pred = predict(logitmodel, data[,1:4], type='response')
pred = predict(logitmodel.model, data[,1:4], type='response')
pred = predict(logitmodel, data[,1:4], type='response')
pred$class
logitmodel.pred = predict(logitmodel, data[,1:4], type='response')
logitmodel.pred$class
library('pwr')
d <- 5/10
pwr.t.test(d = d, sig.level = 0.05, power = 0.90, alternative = "greater",
type = "one.sample")
pwr.t.test(d = d, sig.level = 0.05, power = 0.95, alternative = "greater",
type = "one.sample")
pwr.t.test(d = d, sig.level = 0.05, power = 0.95, alternative = "less",
type = "one.sample")
pwr.t.test(d = d, sig.level = 0.05, power = 0.95, alternative = "two.sided",
type = "one.sample")
pwr.t.test(d = d, sig.level = 0.05, power = 0.95, alternative = "less",
type = "one.sample")
d <- 5/10
pwr.t.test(d = d, sig.level = 0.05, power = 0.95, alternative = "greater",
type = "one.sample")
#q2
pwr.t.test(d = d, sig.level = 0.05, alternative = "greater",
type = "one.sample")
mean0 = 30
mean1 = 25
alfa = 0.05
beta = 0.05 #beta = 1-power
sd=10
n =  (sd*(qnorm(alfa,lower.tail = TRUE)-(qnorm(1- beta,lower.tail = TRUE)))/(mean1-mean0))^2 # חשוב לשים לב האם בהשערת האפס מסתכלים על זנב שמאלי או ימני
cat("גודל המדגם הדרוש",n) # יש לעגל למעלה
Xc = mean0-(qnorm(alfa,lower.tail = FALSE)*sd)/sqrt👎
Xc = mean0-(qnorm(alfa,lower.tail = FALSE)*sd)/sqrt
cat("size of the test:", n)
Xc = mean0-(qnorm(alfa,lower.tail = FALSE)*sd)/sqrt
qnorm(alfa,lower.tail = FALSE)
(qnorm(alfa,lower.tail = FALSE)*sd)
Xc = mean0-(qnorm(alfa,lower.tail = FALSE)*sd)/sqrt(n)
cat(" K הערך הקריטי :",Xc) # יש לעגל למעלה
cat("Xc (or k) is equal to:", Xc)
Xc = mean0-(qnorm(alfa,lower.tail = TRUE)*sd)/sqrt(n)
cat("Xc (or k) is equal to:", Xc)
Xc = mean0+(qnorm(alfa,lower.tail = TRUE)*sd)/sqrt(n)
cat("Xc (or k) is equal to:", Xc)
